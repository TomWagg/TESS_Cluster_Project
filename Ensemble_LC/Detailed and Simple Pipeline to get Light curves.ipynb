{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from astropy.table import Table, join, MaskedColumn, vstack\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import scipy\n",
    "from astropy.time import Time\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from math import e\n",
    "from math import pi\n",
    "from astropy.table import Column\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "import math\n",
    "from numpy import exp\n",
    "from scipy import integrate\n",
    "from scipy.integrate import quad\n",
    "import pdb\n",
    "import random\n",
    "from scipy import stats\n",
    "from scipy.optimize import curve_fit\n",
    "import scipy.optimize as opt\n",
    "import statsmodels \n",
    "from multiprocessing import Pool\n",
    "from scipy.signal import find_peaks\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "import glob\n",
    "import lightkurve as lk\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "import lightkurve as lk\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from astropy.timeseries import LombScargle\n",
    "from astropy.table import Table\n",
    "\n",
    "from lightkurve.correctors import RegressionCorrector\n",
    "from lightkurve.correctors import DesignMatrix\n",
    "import scipy.linalg\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Light Curves from a Cluster Catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The things that we need from the Cluster Catalog are:\n",
    "    1. The Cluster Name \n",
    "    2. The Cluster Location\n",
    "    3. The Cluster Radius (In degrees)\n",
    "    4. The Cluster Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need the path we want to save the finished light curves to\n",
    "\n",
    "I am going to use the following path throughout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path_to_Save_to= '/Users/Tobin/Dropbox/TESS_project/Variability_Statistics/Test_Pipeline_Module/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Personally, In this folder I have subfolders which may be referenced throughout the rest of the notebook. I will be sure to note when I am using a sub-folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I am going to define a function to get the corrected light curve for a given cluster. There are going to be two products from the function. One is the corrected light curve, and the other is a astropy table which deatils output and cluster information. \n",
    "\n",
    "The function will depend on the callable I want to use for the cluster (i.e name or location). This will be a decision put into the call_name_type. By default, I have set it to be Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Way I have set up the function is to split up specific tasks into individual functions, and then have a final function which combines the steps and if statements to get the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The First thing we are going to do is see if the Light Curve output table already exists in our Path. Using the output table because even if the lightcurve ins't available or high enough quality, the output table will say so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def previously_downloaded(Cluster_name):   \n",
    "    sub_path= 'Corrected_LCs/' #A sub-folder in my Path\n",
    "    return os.path.exists(Path_to_Save_to+sub_path+str(Cluster_name)+'output_table.fits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing we want to do is see if TESS has observed the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tess_data(Callable):\n",
    "    CLUSTERS = [str(Callable)]\n",
    "\n",
    "    search = lk.search_tesscut(CLUSTERS[0])#search for the cluster in TESS using lightkurve\n",
    "\n",
    "    print(CLUSTERS[0]+str(' Has ')+str(len(search))+str(' Observations'))\n",
    "\n",
    "    if len(search) > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Necessary Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversion Equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degs_to_pixels(degs):\n",
    "    return degs*60*60/21 #convert degrees to arcsecs and then divide by the resolution of TESS (21 arcsec per pixel)\n",
    "\n",
    "def pixels_to_degs(pixels):\n",
    "    return pixels*21/(60*60) #convert degrees to arcsecs and then divide by the resolution of TESS (21 arcsec per pixel)\n",
    "\n",
    "def flux_to_mag(flux):\n",
    "    m1=10    \n",
    "    f1=15000    \n",
    "    mag=2.5*(np.log10(f1/flux)) + m1    \n",
    "    return mag\n",
    "\n",
    "def flux_err_to_mag_err(flux, flux_err):\n",
    "    d_mag_d_flux= -2.5/(flux*np.log(10))\n",
    "\n",
    "    m_err_squared=(abs(d_mag_d_flux)**2)*(flux_err**2)\n",
    "\n",
    "    return np.sqrt(m_err_squared)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following 2 function have been adapted from Vijith to define which pixels in the cluster aperature should be included or excluded in the aperature\n",
    "\n",
    "These function mark one of the biggest deviations from single star fitting, as we are defining a circle aperature opposed to assignming a flux threshold cut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circle_aperture(data,bkg,radius,PERCENTILE):\n",
    "    radius_in_pixels=degs_to_pixels(radius)\n",
    "    data_mask = np.zeros_like(data)\n",
    "    x_len=np.shape(data_mask)[1]\n",
    "    y_len=np.shape(data_mask)[2]\n",
    "    #centers\n",
    "    cen_x=x_len//2\n",
    "    cen_y=y_len//2\n",
    "    bkg_mask = np.zeros_like(bkg)\n",
    "    bkg_cutoff = getUpperLimit(bkg,PERCENTILE)\n",
    "    for i in range(x_len):\n",
    "        for j in range(y_len):\n",
    "            if (i-cen_x)**2+(j-cen_y)**2<(radius_in_pixels)**2:# star mask condition\n",
    "                data_mask[0,i,j]=1\n",
    "\n",
    "    x_len=np.shape(bkg_mask)[1]\n",
    "    y_len=np.shape(bkg_mask)[2]\n",
    "    cen_x=x_len//2\n",
    "    cen_y=y_len//2\n",
    "    for i in range(x_len):\n",
    "        for j in range(y_len):\n",
    "            if np.logical_and((i-cen_x)**2+(j-cen_y)**2>(radius_in_pixels)**2, bkg[0,i,j]<bkg_cutoff): # sky mask condition\n",
    "                bkg_mask[0,i,j]=1            \n",
    "\n",
    "    star_mask = data_mask==1\n",
    "    sky_mask = bkg_mask==1\n",
    "\n",
    "    return star_mask,sky_mask# return masks\n",
    "    \n",
    "def getUpperLimit(dataDistribution,PERCENTILE):\n",
    "    if UPPER_LIMIT_METHOD == 1:\n",
    "        return np.nanpercentile(dataDistribution, PERCENTILE)\n",
    "\n",
    "    elif UPPER_LIMIT_METHOD == 2:\n",
    "        hist = np.histogram(dataDistribution, bins=BINS, range=(0, 3000))# Bin the data\n",
    "        return hist[1][np.argmax(hist[0])]# Return the flux corresponding to the most populated bin\n",
    "\n",
    "    elif UPPER_LIMIT_METHOD == 3:\n",
    "        pass\n",
    "\n",
    "    elif UPPER_LIMIT_METHOD == 4:\n",
    "        numMaxima = countMaxima(tpfs[i][frame].flux.reshape((cutout_size, cutout_size)))\n",
    "        numPixels = np.count_nonzero(~np.isnan(tpfs[i][frame].flux))\n",
    "        return np.nanpercentile(dataDistribution, 100 - numMaxima / numPixels * 100)\n",
    "\n",
    "    else:\n",
    "        return 150\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are **choices** which we have adopted. The PERCENTILE limit is particularly important. The cutout size can be adjsuted based on the distance to the clusters we want. For example, extragalactic clusters can have a smaller cutout size to save time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "UPPER_LIMIT_METHOD = 1\n",
    "PERCENTILE = 85\n",
    "cutout_size= 99 #Max for unknown reasons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Checks\n",
    "\n",
    "The next phase will be to download the data, perform quality checks and get the uncorrected light curves\n",
    "\n",
    "Most clusters have more than one observation, we are going to move throught the observations until we find one that passes all of our quality flags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few scenarios where we will move on from the current observation, and move to the next sector observation.\n",
    "    1. There is TESS data, but it is unable to be downloaded. (This occurs for a very small number of Sectors, and I don't know why.) \n",
    "    \n",
    "Here is a small test to see if we can download the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadable(Callable, current_try_sector):   \n",
    "    use_name=[Callable]\n",
    "    #Using a Try statement to see if we can download the cluster data\n",
    "    try:\n",
    "        tpfs=lk.search_tesscut(use_name[0])[current_try_sector].download(cutout_size=(cutout_size, cutout_size))\n",
    "    except:\n",
    "        print(\"No Download\")\n",
    "        return 'Bad'\n",
    "\n",
    "    return 'Fine'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    2. The Cluster is near the edge of the observation. (When this happens, there are nan values in the lightcurve which are unnecessarily hard to deal with, so we will only select clusters that are *not* close to the edges.)\n",
    "    3. Sector 1 has a known spacecraft systematic, which I haven't been able to correct out of our light curves, so we will ignore this sector)\n",
    "    \n",
    "These 3 scenarios are combined into the following test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test_near_edge(tpfs): \n",
    "\n",
    "    t1= tpfs[np.where(tpfs.to_lightcurve().quality==0)] #Only selecting time steps that are good, or have quality =0 \n",
    "    #Also making sure the Sector isn't the one with the Systematic\n",
    "    if (np.isnan(np.min(t1[0].flux.value)) == False) & (tpfs.sector != 1) & (np.min(t1[0].flux.value) > 1):\n",
    "        return 'fine'\n",
    "    else: \n",
    "        return 'Bad'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4. In the Observation, there is significant scatted light which comes from sources not in the cluster. This can be the moon/earth, or a nearby bright star. We calculate this by fitting a model plane to the flux values across the image. If there is scattered light, there will be a gradiant across the image which we quantify by the coefficients of the model plane. \n",
    "> * The term *Significant* Is fairly arbitrary. It comes from 27 hand selected observations of 10 clusters, where I plotted the observations that had scattered light vs. the one that didn't, and drew a threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test_for_Scattered_Light(use_tpfs, full_model_Normalized):\n",
    "    # regular grid covering the domain of the data\n",
    "    X,Y = np.meshgrid(np.arange(0, cutout_size, 1), np.arange(0, cutout_size, 1))\n",
    "    XX = X.flatten()\n",
    "    YY = Y.flatten()\n",
    "\n",
    "    #Define the steps for which we test for scattered light \n",
    "    time_steps=np.arange(0,len(use_tpfs), 5) #Set the search to be for every 5th time step, can be changed based of the believed frequency of the scattered light\n",
    "\n",
    "    coefficients_array=np.zeros((len(time_steps), 3))\n",
    "\n",
    "    data_flux_values=(use_tpfs-full_model_Normalized).flux.value\n",
    "\n",
    "    for i in range(len(time_steps)):\n",
    "        data=data_flux_values[time_steps[i]]\n",
    "        # best-fit linear plane\n",
    "        A = np.c_[XX, YY, np.ones(XX.shape)]\n",
    "        C,_,_,_ = scipy.linalg.lstsq(A, data.flatten())    # coefficients\n",
    "        coefficients_array[i]=C\n",
    "\n",
    "        #Deleting defined items we don't need any more to save memory\n",
    "        del A\n",
    "        del C\n",
    "        del data\n",
    "\n",
    "    X_cos= coefficients_array[:,0]\n",
    "    Y_cos= coefficients_array[:,1]\n",
    "    Z_cos= coefficients_array[:,2]\n",
    "\n",
    "    mxc=max(abs(X_cos))\n",
    "    myc=max(abs(Y_cos))\n",
    "    mzc=max(abs(Z_cos))\n",
    "\n",
    "    #Deleting defined items we don't need any more to save memory\n",
    "    del X_cos\n",
    "    del Y_cos\n",
    "    del Z_cos\n",
    "    del coefficients_array\n",
    "    gc.collect() #This is a command which will delete stray arguments to save memory\n",
    "\n",
    "    if (mzc > 1.0) | ((mxc > 0.02) & (myc > 0.02)): \n",
    "        return 'Bad'\n",
    "    else:\n",
    "        return 'Fine'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correct the Light Curve\n",
    "\n",
    "#### At the same time, move through Observations until we find one that passes each quality flag\n",
    "\n",
    "This function is going to combine many of the previous functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function and corrections steps adapt certain decisions, but generlly follow the tutorial given here: https://tessgi.github.io/tess-case-studies/notebooks/pixel-level-detrending/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_LC(Callable, Radius, Cluster_name):\n",
    "    # Knowing how many observations we have to work with\n",
    "    search = lk.search_tesscut(Callable)\n",
    "    sectors_available=len(search)\n",
    "    \n",
    "    #We are also going to doccument how many observations failed each one of our quality tests\n",
    "    failed_download=0\n",
    "    near_edge_or_Sector_1=0\n",
    "    Scattered_Light=0\n",
    "\n",
    "    #start interating through the observations until I find a good one        \n",
    "    for current_try_sector in range(sectors_available):\n",
    "        print(\"Starting Quality Tests for Observation:\", current_try_sector)\n",
    "\n",
    "        if downloadable(Callable, current_try_sector)== 'Bad':\n",
    "            print('Failed Download')\n",
    "            failed_download=failed_download+1\n",
    "            continue\n",
    "        else:\n",
    "            use_name=[Callable]\n",
    "            tpfs=tpfs=lk.search_tesscut(use_name[0])[current_try_sector].download(cutout_size=(cutout_size, cutout_size))\n",
    "            \n",
    "        if Test_near_edge(tpfs) == 'Bad':\n",
    "            print('Failed Near Edge Test')\n",
    "            near_edge_or_Sector_1=near_edge_or_Sector_1+1\n",
    "            continue\n",
    "        else: \n",
    "            use_tpfs = tpfs[np.where(tpfs.to_lightcurve().quality==0)]\n",
    "\n",
    "        #Getting Rid of where the flux err < 0\n",
    "        use_tpfs = use_tpfs[use_tpfs.to_lightcurve().flux_err>0]\n",
    "\n",
    "        #Define the aperature for our Cluster based on previous Vijith functions\n",
    "        star_mask1=np.empty([len(use_tpfs),cutout_size,cutout_size],dtype='bool')\n",
    "        sky_mask1=np.empty([len(use_tpfs),cutout_size,cutout_size],dtype='bool')\n",
    "\n",
    "        star_mask1[0],sky_mask1[0] = circle_aperture(use_tpfs[0].flux.value, use_tpfs[0].flux.value, Radius, PERCENTILE)\n",
    "\n",
    "        keep_mask=star_mask1[0]\n",
    "        \n",
    "        #This will print an image of the cluster with the used aperature shown in red\n",
    "        p = use_tpfs.plot(frame=use_tpfs.shape[0] // 2, aperture_mask=keep_mask)\n",
    "        \n",
    "        #Now we will begin to correct the lightcurve\n",
    "        \n",
    "        uncorrected_lc = use_tpfs.to_lightcurve(aperture_mask=keep_mask)\n",
    "\n",
    "        # Time average of the pixels in the TPF:\n",
    "        max_frame = use_tpfs.flux.value.max(axis=0)\n",
    "        # This renormalizes any columns which are bright because of straps on the detector\n",
    "        max_frame -= np.median(max_frame, axis=0)\n",
    "        # This aperture is any \"faint\" pixels:\n",
    "        bkg_aper = max_frame < np.percentile(max_frame, PERCENTILE)\n",
    "        # The average light curve of the faint pixels is a good estimate of the scattered light\n",
    "        scattered_light = use_tpfs.flux.value[:, bkg_aper].mean(axis=1)\n",
    "\n",
    "        #We can use our background aperture to create pixel time series and then take Principal Components of the data using Singular Value Decomposition. This gives us the \"top\" trends that are present in the background data.\n",
    "        # I have picked 6 based on previous studies showing that is an abritrarily optimal number of components\n",
    "        pca_dm1 = lk.DesignMatrix(use_tpfs.flux.value[:, bkg_aper], name='PCA').pca(6) \n",
    "        #Here we are going to set the priors for the PCA to be located around the flux values of the uncorected LC\n",
    "        pca_dm1.prior_mu =np.array([np.median(uncorrected_lc.flux.value) for i in range(6)])\n",
    "        pca_dm1.prior_sigma =np.array([(np.percentile(uncorrected_lc.flux.value, 84) - np.percentile(uncorrected_lc.flux.value, 16)) for i in range(6)])\n",
    "\n",
    "        #The TESS mission pipeline provides cotrending basis vectors (CBVs) which capture common trends in the dataset. We can use these to detrend out pixel level data.\n",
    "        #The mission provides MultiScale CBVs, which are at different time scales. In this case, we don't want to use the long scale CBVs, because this may fit out real astrophysical variability. Instead we will use the medium and short time scale CBVs.\n",
    "        \n",
    "        cbvs_1 = lk.correctors.cbvcorrector.download_tess_cbvs(sector=use_tpfs.sector, camera=use_tpfs.camera, ccd=use_tpfs.ccd, cbv_type='MultiScale', band=2).interpolate(use_tpfs.to_lightcurve())\n",
    "        cbvs_2 = lk.correctors.cbvcorrector.download_tess_cbvs(sector=use_tpfs.sector, camera=use_tpfs.camera, ccd=use_tpfs.ccd, cbv_type='MultiScale', band=3).interpolate(use_tpfs.to_lightcurve())\n",
    "\n",
    "\n",
    "        cbv_dm1 = cbvs_1.to_designmatrix(cbv_indices=np.arange(1, 8))\n",
    "        cbv_dm2 = cbvs_2.to_designmatrix(cbv_indices=np.arange(1, 8))\n",
    "\n",
    "        # This combines the different timescale CBVs into a single `designmatrix` object\n",
    "        cbv_dm_use = lk.DesignMatrixCollection([cbv_dm1, cbv_dm2]).to_designmatrix()\n",
    "        \n",
    "        # We can make a simple basis-spline (b-spline) model for astrophysical variability. This will be a flexible, smooth model.\n",
    "        # The number of knots is important, we want to only correct for very long term variabilty that looks like systematics, so here we have 5 knots, the smaller the better\n",
    "        spline_dm1 = lk.designmatrix.create_spline_matrix(use_tpfs.time.value, n_knots=5)\n",
    "\n",
    "        \n",
    "        # Here we create our design matrix\n",
    "\n",
    "        dm1 = lk.DesignMatrixCollection([pca_dm1,\n",
    "                                         cbv_dm_use,\n",
    "                                         spline_dm1,\n",
    "                                         ])\n",
    "\n",
    "\n",
    "        full_model, systematics_model, full_model_Normalized = np.ones((3, *use_tpfs.shape))\n",
    "        for idx in tqdm(range(use_tpfs.shape[1])):\n",
    "            for jdx in range(use_tpfs.shape[2]):\n",
    "                pixel_lightcurve = lk.LightCurve(time=use_tpfs.time.value, flux=use_tpfs.flux.value[:, idx, jdx], flux_err=use_tpfs.flux_err.value[:, idx, jdx])\n",
    "\n",
    "                #Adding a test to make sure ther are No Flux_err's <= 0\n",
    "                pixel_lightcurve=pixel_lightcurve[pixel_lightcurve.flux_err > 0]\n",
    "\n",
    "                r1 = lk.RegressionCorrector(pixel_lightcurve)\n",
    "                # Correct the pixel light curve by our design matrix\n",
    "                r1.correct(dm1)\n",
    "                # Extract just the systematics components\n",
    "                systematics_model[:, idx, jdx] = (r1.diagnostic_lightcurves['PCA'].flux.value +\n",
    "                                                  r1.diagnostic_lightcurves['CBVs'].flux.value)\n",
    "                # Add all the components\n",
    "                full_model[:, idx, jdx] =  (r1.diagnostic_lightcurves['PCA'].flux.value +\n",
    "                                            \n",
    "                                            r1.diagnostic_lightcurves['CBVs'].flux.value +\n",
    "\n",
    "                                            r1.diagnostic_lightcurves['spline'].flux.value)\n",
    "\n",
    "                #Making so the model isn't centered around 0\n",
    "                full_model[:, idx, jdx] -= r1.diagnostic_lightcurves['spline'].flux.value.mean()\n",
    "\n",
    "\n",
    "                #Making Normalized Model For the Test of Scattered Light\n",
    "                full_model_Normalized[:, idx, jdx] =  (r1.diagnostic_lightcurves['PCA'].flux.value +\n",
    "                                                       \n",
    "                                                       r1.diagnostic_lightcurves['CBVs'].flux.value +\n",
    "\n",
    "                                                       r1.diagnostic_lightcurves['spline'].flux.value)                       \n",
    "\n",
    "        #Calculate Lightcurves\n",
    "#NOTE- we are also calculating a lightcurve which does not include the spline model, this is the systematics_model_corrected_lightcurve1\n",
    "        scattered_light_model_correected_lightcurve=(use_tpfs - scattered_light[:, None, None]).to_lightcurve(aperture_mask=keep_mask)\n",
    "        systematics_model_corrected_lightcurve=(use_tpfs - systematics_model).to_lightcurve(aperture_mask=keep_mask)\n",
    "        full_corrected_lightcurve=(use_tpfs - full_model).to_lightcurve(aperture_mask=keep_mask)\n",
    "\n",
    "        full_corrected_lightcurve_table=Table([full_corrected_lightcurve.time.value, \n",
    "                                                full_corrected_lightcurve.flux.value,\n",
    "                                                full_corrected_lightcurve.flux_err.value], \n",
    "                                                names=('time', 'flux', 'flux_err'))\n",
    "        systematics_model_corrected_lightcurve_table=Table([systematics_model_corrected_lightcurve.time.value, \n",
    "                                                systematics_model_corrected_lightcurve.flux.value,\n",
    "                                                systematics_model_corrected_lightcurve.flux_err.value], \n",
    "                                                names=('time', 'flux', 'flux_err'))\n",
    "\n",
    "\n",
    "        if (Test_for_Scattered_Light(use_tpfs, full_model_Normalized) == 'Bad') & (current_try_sector+1 < sectors_available):\n",
    "            print(\"Failed Scattered Light Test\")\n",
    "            Scattered_Light=Scattered_Light+1\n",
    "            continue\n",
    "        if (Test_for_Scattered_Light(use_tpfs, full_model_Normalized) == 'Bad') & (current_try_sector+1 == sectors_available):\n",
    "            print(\"Failed Scattered Light Test\")\n",
    "            Scattered_Light=Scattered_Light+1\n",
    "            return ['No Good Observations'], ['Nothing'] , ['Nothing'], np.array(int(sectors_available)), np.array(int(failed_download)), np.array(int(near_edge_or_Sector_1)), np.array(int(Scattered_Light))\n",
    "        else:\n",
    "            print(current_try_sector, \"Passed Quality Tests\")\n",
    "            #This Else Statement means that the Lightcurve is good and has passed our quality checks\n",
    "            break\n",
    "    return full_corrected_lightcurve_table, systematics_model_corrected_lightcurve_table, np.array(int(current_try_sector)), np.array(int(sectors_available)), np.array(int(failed_download)), np.array(int(near_edge_or_Sector_1)), np.array(int(Scattered_Light))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put it All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Corrected_Lightcurve(Cluster_name, Location, Radius, Cluster_Age, call_name_type_Name=True, save_figs=True, print_figs=True):\n",
    "    \n",
    "    #Setting which collable we want to use\n",
    "    if call_name_type_Name:\n",
    "        Callable= Cluster_name\n",
    "    else:\n",
    "        Callable= Location\n",
    "        \n",
    "    if tess_data(Callable) == True:\n",
    "        #Test to see if I have already downloaded and corrected this cluster, If I have, read in the data\n",
    "        if previously_downloaded(Cluster_name) == True:\n",
    "            output_table= Table.read(Path_to_Save_to+'Corrected_LCs/'+str(Cluster_name)+'output_table.fits')\n",
    "            if output_table['Used_Ob'][0] != -99: # -99 in the Used Observation Column means there were no good observations\n",
    "                data= Table.read(Path_to_Save_to+'Corrected_LCs/'+str(Cluster_name)+'.fits')\n",
    "                systematic_model_data= Table.read(Path_to_Save_to+'Corrected_LCs/'+str(Cluster_name)+'ONLY_SYSMOD.fits')\n",
    "                \n",
    "                #Quickly remaking the Light curve plot as an output\n",
    "                range_=max(data['flux'])-min(data['flux'])\n",
    "                fig=plt.figure()\n",
    "                plt.plot(data['time'], data['flux'], color='k', linewidth=.5)\n",
    "                plt.xlabel('Delta Time [Days]')\n",
    "                plt.ylabel('Flux [e/s]')\n",
    "                plt.text(data['time'][0], max(data['flux']), str(Cluster_name), fontsize=16)\n",
    "                plt.text(data['time'][0], (max(data['flux'])-(range_*.1)),\n",
    "                         (str(\"Age:\")+str('{0:.2g}').format(Cluster_Age)), fontsize=16)\n",
    "\n",
    "                if print_figs:\n",
    "                    plt.show()\n",
    "                plt.close(fig)\n",
    "                \n",
    "                return data, output_table, fig\n",
    "                      \n",
    "            else: #When I downloaded it before, there were no good Observations\n",
    "                data=['No Good Observations'] \n",
    "                \n",
    "                return data, output_table, 'NA'\n",
    "                              \n",
    "        \n",
    "        else: # This else statement refers to the Cluster Not Previously Being Downloaded          \n",
    "              # So Calling funcion to download and correct data\n",
    "            data, systematic_model_data, used_observation, Obs_Available, Obs_failed_download, Obs_near_Edge, Obs_Scattered_Light = Get_LC(Callable, Radius, Cluster_name)\n",
    "        \n",
    "        # Now that I have my data, if it is a light curve, I'm going to make the figure, and \n",
    "        if data[0] != 'No Good Observations':\n",
    "            data.add_column(Column(flux_to_mag(data['flux'])), name='mag')\n",
    "            data.add_column(Column(flux_err_to_mag_err(data['flux'], data['flux_err'])), name='mag_err')\n",
    "\n",
    "            #Making the Output Table\n",
    "            name___=[Cluster_name]\n",
    "            HTD=[True]\n",
    "            OB_use=[used_observation]\n",
    "            OB_av=[Obs_Available]\n",
    "            OB_fd=[Obs_failed_download]\n",
    "            OB_ne=[Obs_near_Edge]\n",
    "            OB_sl=[Obs_Scattered_Light]\n",
    "\n",
    "\n",
    "            output_table=Table([name___, [Location], [Radius], [Cluster_Age], HTD, OB_use, OB_av, OB_fd, OB_ne, OB_sl],\n",
    "                               names=('Name', 'Location', 'Radius [deg]', 'Log Age', 'Has_TESS_Data', 'Used_Ob',\n",
    "                                      'Obs_Available', 'Obs_DL_Failed', 'Obs_Near_Edge_S1', 'Obs_Scattered_Light'))\n",
    "\n",
    "            #Writting out the data, so I never have to Download and Correct again, but only if there is data\n",
    "            lc_path=\"Corrected_LCs/\"        \n",
    "            data.write(Path_to_Save_to+lc_path+str(Cluster_name)+'.fits')\n",
    "            systematic_model_data.write(Path_to_Save_to+lc_path+str(Cluster_name)+'ONLY_SYSMOD.fits')\n",
    "            output_table.write(Path_to_Save_to+lc_path+str(Cluster_name)+'output_table.fits')\n",
    "\n",
    "            #Now I am going to save a plot of the light curve to go visually inspect later\n",
    "            range_=max(data['flux'])-min(data['flux'])\n",
    "            fig=plt.figure()\n",
    "            plt.plot(data['time'], data['flux'], color='k', linewidth=.5)\n",
    "            plt.xlabel('Delta Time [Days]')\n",
    "            plt.ylabel('Flux [e/s]')\n",
    "            plt.text(data['time'][0], max(data['flux']), str(Cluster_name), fontsize=16)\n",
    "            plt.text(data['time'][0], (max(data['flux'])-(range_*.1)),\n",
    "                     (str(\"Age:\")+str('{0:.2g}').format(Cluster_Age)), fontsize=16)\n",
    "\n",
    "            path=\"Figures/LCs/\" #Sub-folder \n",
    "            which_fig=\"_Full_Corrected_LC\"\n",
    "            out=\".png\"\n",
    "\n",
    "            if save_figs:\n",
    "                plt.savefig(Path_to_Save_to+path+str(Cluster_name)+which_fig+out, format='png') \n",
    "            if print_figs:\n",
    "                plt.show()\n",
    "            plt.close(fig) \n",
    "\n",
    "            range_=max(systematic_model_data['flux'])-min(systematic_model_data['flux'])\n",
    "            fig2=plt.figure()\n",
    "            plt.plot(systematic_model_data['time'], systematic_model_data['flux'], color='k', linewidth=.5)\n",
    "            plt.xlabel('Delta Time [Days]')\n",
    "            plt.ylabel('Flux [e/s]')\n",
    "            plt.text(systematic_model_data['time'][0], max(systematic_model_data['flux']), str(Cluster_name), fontsize=16)\n",
    "            plt.text(systematic_model_data['time'][0], (max(systematic_model_data['flux'])-(range_*.1)),\n",
    "                     (str(\"Age:\")+str('{0:.2g}').format(Cluster_Age)), fontsize=16)\n",
    "\n",
    "            which_fig2=\"Only_Systematic_Model_Corrected_LC\"\n",
    "\n",
    "            if save_figs:\n",
    "                plt.savefig(Path_to_Save_to+path+str(Cluster_name)+which_fig2+out, format='png') \n",
    "            plt.close(fig2) \n",
    "            \n",
    "            return data, output_table, fig\n",
    "                \n",
    "            \n",
    "        else: #This else statement refers to there being No good Observtions\n",
    "\n",
    "            used_observation= -99\n",
    "\n",
    "            #Making the Output Table\n",
    "            name___=[Cluster_name]\n",
    "            HTD=[True]\n",
    "            OB_use=[used_observation]\n",
    "            OB_av=[Obs_Available]\n",
    "            OB_fd=[Obs_failed_download]\n",
    "            OB_ne=[Obs_near_Edge]\n",
    "            OB_sl=[Obs_Scattered_Light]\n",
    "\n",
    "\n",
    "            output_table=Table([name___, [Location], [Radius], [Cluster_Age], HTD, OB_use, OB_av, OB_fd, OB_ne, OB_sl],\n",
    "                               names=('Name', 'Location', 'Radius [deg]', 'Log Age', 'Has_TESS_Data', 'Used_Ob',\n",
    "                                      'Obs_Available', 'Obs_DL_Failed', 'Obs_Near_Edge_S1', 'Obs_Scattered_Light'))\n",
    "            \n",
    "            output_table.write(Path_to_Save_to+\"Corrected_LCs/\"+str(Cluster_name)+'output_table.fits')\n",
    "            \n",
    "            return data, output_table, 'NA'\n",
    "          \n",
    "    else: #This Means that there is no TESS coverage for the Cluster (Easiest to check)\n",
    "        name___= [Cluster_name]       \n",
    "        HTD=[False]  \n",
    "        OB_use= np.ma.array([0], mask=[1])\n",
    "        OB_av= np.ma.array([0], mask=[1])\n",
    "        OB_fd= np.ma.array([0], mask=[1])\n",
    "        OB_ne= np.ma.array([0], mask=[1])\n",
    "        OB_sl= np.ma.array([0], mask=[1])        \n",
    "\n",
    "        output_table=Table([name___, [Location], [Radius], [Cluster_Age], HTD, OB_use, OB_av, OB_fd, OB_ne, OB_sl],\n",
    "                           names=('Name', 'Location', 'Radius [deg]', 'Log Age', 'Has_TESS_Data', 'Used_Ob',\n",
    "                                  'Obs_Available', 'Obs_DL_Failed', 'Obs_Near_Edge_S1', 'Obs_Scattered_Light'))\n",
    "\n",
    "        output_table.write(Path_to_Save_to+\"Corrected_LCs/\"+str(Cluster_name)+'output_table.fits', Overwrite=True)\n",
    "\n",
    "        return 'No TESS DATA', output_table, 'NA'\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NGC 419 Has 4 Observations\n",
      "Starting Quality Tests for Observation: 0\n",
      "Failed Near Edge Test\n",
      "Starting Quality Tests for Observation: 1\n",
      "Failed Near Edge Test\n",
      "Starting Quality Tests for Observation: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/99 [09:47<14:23, 14.63s/it]"
     ]
    }
   ],
   "source": [
    "Get_Corrected_Lightcurve(Cluster_name='NGC 419', Location='23.58271, +61.1236', Radius=.046, Cluster_Age=7.75)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
